# -*- coding: utf-8 -*-
"""Taxi_Fare.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16PxyGhE3PONl0hBnnpi9kaQ38z21sQyD
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("TaxiFare.csv")

"""PRE-PROCESS THE DATASET"""

df.head()

df.shape

df.info()

df.describe()

df.columns

df=df.drop(['unique_id'],axis=1)

df.head()

df.dtypes

df.info()

df.describe()

"""**Filling Missing values**"""

df.isnull()

df.isnull().sum()

df.isnull().sum().sum()

df.dtypes

df.date_time_of_pickup   =pd.to_datetime(df.date_time_of_pickup, errors ='coerce',utc=True)

df.dtypes

df.head()

"""**To segregate each time of date and time**"""

df['hour'] = df.date_time_of_pickup.dt.hour
df['day'] = df.date_time_of_pickup.dt.day
df['month'] = df.date_time_of_pickup.dt.month
df['year'] = df.date_time_of_pickup.dt.year
df['dayofweek'] = df.date_time_of_pickup.dt.dayofweek
df['dayName'] = df.date_time_of_pickup.dt.day_name()

df.head()

df.describe()#only for quantitative column

x=df['amount'].value_counts()#sorted in decresing order

print(type(x))
print("Max count = ",x[6.50])
x

df = df.drop('date_time_of_pickup',axis=1)

df.head()

df.dtypes

df.drop(["dayName"],axis=1,inplace=True)

"""**Checking outlier and filling them**"""

number_of_columns = len(df.columns)
number_of_columns

df.plot(kind = "box",subplots = False,layout = (7,2),figsize=(15,20)) #boxplot

df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #boxplot

"""**Explanation of clip function**"""

a = np.arange(10,100,10)
a1 = np.clip(a,30,60)
print("Before clipping :",a)
print("After clipping :",a1)

df = df.drop_duplicates()
df

# Elimination of outliers
def remove_outlier(df1,col):
  Q1 = df1[col].quantile(0.25)
  Q2 = df1[col].quantile(0.50)
  Q3 = df1[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_whisker = Q1-1.5*IQR
  upper_whisker = Q3+1.5*IQR
  print("............................................................")
  print("col=",col,"Q1=",Q1,"Q2=",Q2,"Q3=",Q3)
  print("............................................................")
  df1[col] = np.clip(df1[col],lower_whisker,upper_whisker)
  return df1

def treat_outliers_all(df1 , col_list):
  print("col_list",col_list)
  for c in col_list:
    df1 = remove_outlier(df1,c)
  return  df1

df = treat_outliers_all(df , df.columns)

df.plot(kind = "box",subplots = True,layout = (7,2),figsize=(15,20)) #boxplot

incorrect_coordinates = df.loc[(df.latitude_of_dropoff > 90) | (df.latitude_of_dropoff < -90) |
                                   (df.latitude_of_dropoff  > 90) | (df.latitude_of_dropoff < -90) |
                                   (df.longitude_of_pickup > 180 )| (df.longitude_of_pickup < -180) |
                                   (df.longitude_of_pickup  > 90) | (df.longitude_of_pickup < -90)

                                 ]

incorrect_coordinates

df.drop(incorrect_coordinates,inplace = True , errors = 'ignore')
df

!pip install haversine

import haversine as hs
travel_dist = []
for pos in range(len(df['longitude_of_pickup'])):
  long1,lati1,long2,lati2 = [df['longitude_of_pickup'][pos],df['latitude_of_pickup'][pos],df['longitude_of_dropoff'][pos],df['latitude_of_dropoff'][pos]]
  loc1=(lati1,long1)
  loc2=(lati2,long2)
  c = hs.haversine(loc1,loc2)
  travel_dist.append(c)

print(travel_dist)
df['dist_travel_km'] = travel_dist
df.head()

#Taxi does not  travel over 130 kms
df = df[(df.dist_travel_km >= 1) & (df.dist_travel_km <= 130)]
print("Remaining observations in the dataset :",df.shape)

bool_df = df.isnull()
bool_df

sns.heatmap(bool_df) # no correlation exists since r=0

sns.heatmap(bool_df.corr())

corr = df.corr() #Function to find correlation if r approx 1 than its stronly correlated
corr

plt.subplots(figsize = (12,8))
sns.heatmap(df.corr(),annot = True)# correlation heatmap (light values means highly correlated)

"""# ** Diving the dataset into feature and target values**


"""

df.columns

x = df[df.columns[1:]]#0th column is amount( all columns except amount column)
x

y = df['amount']
y

"""**Diving the dataset into training and testing dataset**"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.70,random_state=7)#30% for training ,70% for testing or viceversa

x_train

x_test

y_train

y_test

len(y) # 100% entries

len(y_train)# 30% for training

len(y_test) # 70% for testing

len(y_train) + len(y_test) # total

"""**Linear Regression**"""

from sklearn.linear_model import LinearRegression
regression = LinearRegression()

regression.fit(x_train,y_train)

regression.intercept_

regression.coef_

for i in range(0,len(regression.coef_)):
  print("theta",i,"_",regression.coef_[i])

y_pred = regression.predict(x_test)# to predict the target values

camparision = pd.DataFrame({"Actual Label":y_test,"predicted Label":y_pred})
camparision

camparision.reset_index()

camparision.reset_index().drop(["index"],axis=1)

sns.heatmap(camparision.corr())

"""**Metrics Evaluation using R2,Mean squared Error,Root Mean Squared Error**"""

from sklearn.metrics import r2_score

r2_score(y_test,y_pred)

from sklearn.metrics import mean_squared_error

MSE = mean_squared_error(y_test,y_pred)
MSE

RMSE = np.sqrt(MSE)
RMSE

"""**Random Forest Regression**"""

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100)

rf.fit(x_train,y_train)

y_pred = rf.predict(x_test)
y_pred

"""**Metrics evaluation for Random forest**"""

R2_Random = r2_score(y_test,y_pred)
R2_Random

MSE_Random = mean_squared_error(y_test,y_pred)
MSE_Random

RMSE_Random = np.sqrt(MSE_Random)
RMSE

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.70,random_state=7)
y=df[['amount']]
x=df.drop(['amount'],axis=1)

model_log=LinearRegression( n_jobs=1, fit_intercept=True)
model_log.fit(x_train,y_train)

model_log.score(x_train,y_train)

model_log.score(x_test,y_test)

from sklearn.tree import DecisionTreeRegressor

model_dtr=DecisionTreeRegressor(max_depth=6,min_weight_fraction_leaf=0.01,criterion='squared_error')

model_dtr.fit(x_train,y_train)

model_dtr.score(x_train,y_train)

model_dtr.score(x_test,y_test)

from sklearn.ensemble import BaggingRegressor
model_bcl=BaggingRegressor(n_estimators=120,max_samples=0.9,base_estimator=model_dtr)
model_bcl.fit(x_train, y_train)
print(model_bcl.score(x_train, y_train))
model_bcl.score(x_test, y_test)

from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor

model_abc=AdaBoostRegressor(n_estimators=10, random_state=0 ,learning_rate=0.1)
model_gbc=GradientBoostingRegressor( )
model_rfc=RandomForestRegressor(n_estimators=120,min_samples_leaf= 20,max_features=20)

model_abc.fit(x_train,y_train)

model_abc.score(x_train,y_train)

model_abc.score(x_test,y_test)

model_gbc.fit(x_train,y_train)

model_gbc.score(x_train,y_train)

model_gbc.score(x_test,y_test)

model_rfc.fit(x_train,y_train)

model_rfc.score(x_train,y_train)

model_rfc.score(x_test,y_test)